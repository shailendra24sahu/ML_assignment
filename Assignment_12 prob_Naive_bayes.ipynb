{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c9c5059",
   "metadata": {},
   "source": [
    "Q1. What is prior probability? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5eb20a",
   "metadata": {},
   "source": [
    "Prior probability, in the context of probability theory and statistics, refers to the initial probability assigned to an event or hypothesis before any specific evidence or information is taken into account. It represents our belief or knowledge about the likelihood of an event occurring based on general information or previous experience.\n",
    "\n",
    "An example of prior probability can be seen in medical diagnostics. Let's consider a hypothetical scenario where a certain disease, let's call it Disease X, affects approximately 1 in 10,000 people in a population. In this case, the prior probability of an individual having Disease X would be 1/10,000, or 0.01%.\n",
    "\n",
    "Now, suppose a person visits a doctor's office with some symptoms that may be associated with Disease X. Before any tests or additional information are considered, the initial belief about the likelihood of that person having the disease is represented by the prior probability of 0.01%. This probability is based on general knowledge about the disease prevalence in the population.\n",
    "\n",
    "The prior probability acts as a starting point, and as more evidence or information becomes available, it can be updated using Bayes' theorem to calculate a posterior probability that takes into account both the prior probability and the new evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3dee95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23bc3706",
   "metadata": {},
   "source": [
    "Q2. What is posterior probability? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf29cd3",
   "metadata": {},
   "source": [
    "Posterior probability, in the context of probability theory and statistics, refers to the updated probability of an event or hypothesis after considering specific evidence or information. It is calculated using Bayes' theorem, which combines the prior probability and the likelihood of the evidence given the hypothesis to obtain the posterior probability.\n",
    "\n",
    "To illustrate with an example, let's consider a scenario where we are taking a multiple-choice test. we have a 50% chance of knowing the correct answer to any given question, and a 50% chance of guessing randomly. Now suppose we answered a question correctly, and we want to determine the probability that we actually knew the answer (prior probability) given our correct response.\n",
    "\n",
    "Let's assign some values to make the calculation clearer. Suppose there are four possible answers to each question, and we know the correct answer to 25% of the questions (prior probability). The likelihood of answering correctly when we know the answer is 1 (100%), and the likelihood of answering correctly by guessing is 1/4 (25%).\n",
    "\n",
    "Using Bayes' theorem, we can calculate the posterior probability as follows:\n",
    "\n",
    "Posterior probability = (Likelihood of evidence given hypothesis) x (Prior probability) / (Probability of evidence)\n",
    "\n",
    "In this case, the probability of evidence is the probability of answering the question correctly, which is a combination of the likelihoods:\n",
    "\n",
    "Probability of evidence = (Likelihood of answering correctly when we know the answer) x (Prior probability) + (Likelihood of answering correctly by guessing) x (Probability of guessing)\n",
    "\n",
    "Plugging in the values:\n",
    "\n",
    "Probability of evidence = (1) x (0.25) + (0.25) x (0.75) = 0.25 + 0.1875 = 0.4375\n",
    "\n",
    "Now, we can calculate the posterior probability:\n",
    "\n",
    "Posterior probability = (1) x (0.25) / (0.4375) ≈ 0.571\n",
    "\n",
    "So, after considering the evidence of answering the question correctly, our updated belief (posterior probability) is approximately 0.571 or 57.1% that we actually knew the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bdc91f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29abf6d0",
   "metadata": {},
   "source": [
    "Q3. What is likelihood probability? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f27ea0",
   "metadata": {},
   "source": [
    "Likelihood probability, in the context of probability theory and statistics, refers to the probability of observing a specific set of data or evidence given a particular hypothesis or model. It quantifies how well the hypothesis explains or predicts the observed data.\n",
    "\n",
    "To provide an example, let's consider a scenario where we have a bag of colored marbles. The bag contains red, blue, and green marbles in unknown proportions. we want to estimate the probability of drawing a red marble from the bag.\n",
    "\n",
    "Suppose we randomly draw five marbles from the bag and observe that four of them are red and one is blue. The likelihood probability would be the probability of observing this specific outcome (four red marbles and one blue marble) given a hypothesis about the proportion of red, blue, and green marbles in the bag.\n",
    "\n",
    "Let's assume that we have a prior belief that there is an equal probability of each color (red, blue, and green) being drawn, so the prior probability for drawing a red marble is 1/3.\n",
    "\n",
    "To calculate the likelihood probability, we need to specify the probability of each color being drawn. Let's assume the probabilities are as follows: P(red) = 0.6, P(blue) = 0.3, and P(green) = 0.1.\n",
    "\n",
    "The likelihood probability can be calculated by multiplying the probabilities of observing each marble:\n",
    "\n",
    "Likelihood probability = P(observing 4 red, 1 blue) = P(red) * P(red) * P(red) * P(red) * P(blue)\n",
    "                                   = 0.6 * 0.6 * 0.6 * 0.6 * 0.3\n",
    "                                   = 0.05184\n",
    "\n",
    "Therefore, the likelihood probability of observing four red marbles and one blue marble, given the specified probabilities, is approximately 0.05184 or 5.184%.\n",
    "\n",
    "The likelihood probability provides a measure of how likely the observed data is under a specific hypothesis or model. It is often used in conjunction with prior probabilities and Bayes' theorem to update beliefs and make statistical inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2914e956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "024c8415",
   "metadata": {},
   "source": [
    "Q4. What is Naïve Bayes classifier? Why is it named so?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd5fbd8",
   "metadata": {},
   "source": [
    "The Naïve Bayes classifier is a popular machine learning algorithm for classification tasks. It is based on applying Bayes' theorem with a \"naïve\" assumption of independence between the features (predictor variables).\n",
    "\n",
    "The classifier is called \"naïve\" because it assumes that the presence or absence of a particular feature in a class is independent of the presence or absence of other features. This assumption simplifies the computation and makes the algorithm efficient, but it may not hold true in every real-world scenario. Despite this simplifying assumption, Naïve Bayes classifiers have proven to be surprisingly effective in many practical applications.\n",
    "\n",
    "The Naïve Bayes classifier uses the concept of conditional probability to make predictions. Given a set of features (X1, X2, ..., Xn) and a class variable (C), it calculates the conditional probability of each class given the observed features. The class with the highest conditional probability is predicted as the output.\n",
    "\n",
    "Mathematically, the Naïve Bayes classifier can be represented as follows:\n",
    "\n",
    "P(C | X1, X2, ..., Xn) = (P(X1 | C) * P(X2 | C) * ... * P(Xn | C) * P(C)) / P(X1, X2, ..., Xn)\n",
    "\n",
    "In this formula, P(C | X1, X2, ..., Xn) represents the posterior probability of class C given the features X1, X2, ..., Xn. P(Xi | C) is the probability of feature Xi given class C, and P(C) is the prior probability of class C. P(X1, X2, ..., Xn) is the probability of the observed features, which serves as a normalization constant and can be ignored for prediction purposes since it is the same for all classes.\n",
    "\n",
    "The Naïve Bayes classifier is known for its simplicity, efficiency, and ability to handle large feature spaces. It is commonly used in text classification, spam filtering, sentiment analysis, and other tasks where the independence assumption is reasonable or provides satisfactory results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d4ad4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f57c850",
   "metadata": {},
   "source": [
    "Q5. What is optimal Bayes classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7c406f",
   "metadata": {},
   "source": [
    "The Optimal Bayes classifier, also known as the Bayes optimal classifier or Bayes optimal decision rule, is a theoretical concept in machine learning and statistics. It represents the ideal classifier that minimizes the overall error rate or misclassification rate when given complete and accurate knowledge of the underlying probability distributions.\n",
    "\n",
    "The Optimal Bayes classifier achieves this by assigning each instance to the class with the highest posterior probability. In other words, it selects the class that has the highest probability given the observed features. This decision rule is based on Bayes' theorem and provides the best possible classification performance given the available information.\n",
    "\n",
    "Mathematically, the Optimal Bayes classifier can be represented as:\n",
    "\n",
    "Predicted class = argmax [P(C | X1, X2, ..., Xn)]\n",
    "\n",
    "where C represents the class variable, Xi represents the observed features, and P(C | X1, X2, ..., Xn) is the posterior probability of class C given the observed features.\n",
    "\n",
    "To implement the Optimal Bayes classifier in practice, one would need to know the true underlying probability distributions and have complete information about the data. However, in real-world scenarios, this level of knowledge is often unattainable or too costly to acquire.\n",
    "\n",
    "In practice, the Naïve Bayes classifier (discussed in the previous question) is a popular approximation to the Optimal Bayes classifier. It makes the simplifying assumption of feature independence to reduce computational complexity, even though it may not hold true in every situation. Despite this approximation, Naïve Bayes classifiers often achieve good performance on various classification tasks.\n",
    "\n",
    "The Optimal Bayes classifier serves as a theoretical benchmark against which other classifiers are evaluated. It represents the optimal decision-making strategy when perfect knowledge of the underlying probability distributions is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00cdc62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f831eb31",
   "metadata": {},
   "source": [
    "Q6. Write any two features of Bayesian learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3ce7a9",
   "metadata": {},
   "source": [
    "Two features of Bayesian learning methods are:\n",
    "\n",
    "1. Probabilistic Framework: Bayesian learning methods provide a probabilistic framework for modeling and inference. They utilize probability distributions to represent uncertainty and make predictions based on posterior probabilities. Instead of providing a single prediction, Bayesian learning methods provide a probability distribution over possible outcomes, allowing for more nuanced and informative decision-making.\n",
    "\n",
    "2. Prior Knowledge Incorporation: Bayesian learning methods allow for the incorporation of prior knowledge or beliefs into the learning process. Prior probabilities can be assigned to hypotheses or model parameters based on existing information or domain expertise. These priors are combined with observed data through Bayes' theorem to obtain updated posterior probabilities. By incorporating prior knowledge, Bayesian learning methods can improve generalization and handle situations with limited data more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2418d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7094851f",
   "metadata": {},
   "source": [
    "Q7. Define the concept of consistent learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd48948b",
   "metadata": {},
   "source": [
    "In machine learning, a consistent learner is a type of learning algorithm that converges to the true target function or hypothesis as the amount of training data increases. Consistency is a desirable property as it ensures that the learner's predictions become increasingly accurate and approach the true underlying pattern in the data.\n",
    "\n",
    "Formally, a learner is considered consistent if, given an infinite amount of training data, it is guaranteed to converge to the true target function or hypothesis with a probability of 1. In other words, as the sample size increases indefinitely, the learner's predictions become arbitrarily close to the correct predictions.\n",
    "\n",
    "Consistency is often associated with the notion of convergence in learning theory. It implies that as the learner observes more and more training examples, it will ultimately learn the true pattern in the data and make accurate predictions on unseen instances.\n",
    "\n",
    "Consistent learners are important because they provide theoretical guarantees on the quality of predictions as the amount of data increases. These learners assure that with sufficient data, the model will approach the best possible performance achievable given the true target function.\n",
    "\n",
    "It is worth noting that not all learning algorithms are consistent. Some learners may exhibit convergence to a different hypothesis or have limitations that prevent them from achieving consistency, particularly in cases of insufficient or noisy data. Evaluating the consistency of a learning algorithm is a fundamental aspect of assessing its performance and reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcbafed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cf0ea7d",
   "metadata": {},
   "source": [
    "Q8. Write any two strengths of Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de34516",
   "metadata": {},
   "source": [
    "Two strengths of the Bayes classifier are:\n",
    "\n",
    "1. Strong Theoretical Foundation: The Bayes classifier is based on Bayes' theorem and has a strong theoretical foundation in probability theory and statistics. It provides a principled framework for making decisions based on posterior probabilities, taking into account both prior knowledge and observed evidence. The Bayesian approach is well-established and has been extensively studied, allowing for rigorous analysis and interpretation of results.\n",
    "\n",
    "2. Effective Handling of Small Data: The Bayes classifier performs well even when the available training data is limited. By incorporating prior knowledge or beliefs, it can make reasonable predictions even with a small number of observed instances. The Bayesian framework allows for the integration of prior information, which can help regularize the model and improve generalization. This ability to leverage prior knowledge makes the Bayes classifier particularly useful in situations where data scarcity is a challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd51b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "065f9c9d",
   "metadata": {},
   "source": [
    "Q9. Write any two weaknesses of Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8503cd5",
   "metadata": {},
   "source": [
    "Two weaknesses of the Bayes classifier are:\n",
    "\n",
    "1. Assumption of Feature Independence: The Naïve Bayes classifier, a popular variant of the Bayes classifier, assumes that the features (predictor variables) are independent of each other given the class label. This assumption may not hold true in many real-world scenarios, leading to a mismatch between the independence assumption and the actual dependencies in the data. While the Naïve Bayes classifier can still perform well in practice, this assumption can limit its ability to capture complex relationships among features.\n",
    "\n",
    "2. Sensitivity to Incorrect Assumptions: The performance of the Bayes classifier is highly dependent on the accuracy of the underlying assumptions made during modeling. If the assumed probability distributions or prior probabilities do not reflect the true data generation process, the classifier's predictions can be suboptimal or even highly inaccurate. In cases where the assumptions are violated or incorrect, the Bayes classifier may struggle to capture the true patterns in the data, leading to reduced performance. Ensuring the accuracy of assumptions is crucial for obtaining reliable results from the Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd00d09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3718241a",
   "metadata": {},
   "source": [
    "Q10. Explain how Naïve Bayes classifier is used for\n",
    "\n",
    "1. Text classification\n",
    "\n",
    "2. Spam filtering\n",
    "\n",
    "3. Market sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec55fbef",
   "metadata": {},
   "source": [
    "#### 1. Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23d0b96",
   "metadata": {},
   "source": [
    "The Naïve Bayes classifier is commonly used for text classification tasks, where the goal is to assign predefined categories or labels to textual documents based on their content. Here's how the Naïve Bayes classifier is typically used for text classification:\n",
    "\n",
    "1. Data Preprocessing: The textual data is first preprocessed to remove any noise or irrelevant information. This typically involves steps such as tokenization (splitting the text into individual words or tokens), removing punctuation, converting text to lowercase, and handling stop words (common words like \"and,\" \"the,\" etc. that often carry little meaning).\n",
    "\n",
    "2. Feature Extraction: The next step is to represent the textual documents as feature vectors that can be used by the Naïve Bayes classifier. Commonly used techniques for feature extraction include the bag-of-words model or more advanced methods like term frequency-inverse document frequency (TF-IDF) weighting. These techniques transform the text into numerical representations, where each feature corresponds to a word or term and the value represents its frequency or importance in the document.\n",
    "\n",
    "3. Training the Classifier: Once the textual data is preprocessed and transformed into feature vectors, a Naïve Bayes classifier is trained using a labeled training dataset. During training, the classifier estimates the probability distribution of features given each class. In the case of text classification, this means estimating the probabilities of words or terms occurring in each class (e.g., spam or non-spam for email classification).\n",
    "\n",
    "4. Calculating Posterior Probabilities: To classify new or unseen documents, the Naïve Bayes classifier calculates the posterior probabilities of each class given the observed features. It applies Bayes' theorem to compute the probability of a document belonging to each class based on the occurrence of specific words or terms in the document.\n",
    "\n",
    "5. Prediction and Evaluation: Finally, the Naïve Bayes classifier assigns the most probable class label to the document based on the computed posterior probabilities. The predicted class can then be evaluated against the true class label to assess the classifier's performance. Common evaluation metrics include accuracy, precision, recall, and F1 score.\n",
    "\n",
    "The Naïve Bayes classifier's simplicity, efficiency, and ability to handle high-dimensional feature spaces make it a popular choice for text classification tasks. While the assumption of feature independence may not hold in all cases, Naïve Bayes classifiers often perform well in practice and are widely used for tasks like sentiment analysis, spam detection, topic classification, and document categorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dac67c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "669957f9",
   "metadata": {},
   "source": [
    "#### 2. Spam filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d8cf7d",
   "metadata": {},
   "source": [
    "The Naïve Bayes classifier is a popular choice for spam filtering, where the goal is to automatically identify and classify incoming emails as either spam or legitimate (non-spam). Here's how the Naïve Bayes classifier is typically used for spam filtering:\n",
    "\n",
    "1. Data Preparation: A training dataset is prepared consisting of labeled emails, where each email is tagged as either spam or non-spam. These emails are used to train the Naïve Bayes classifier.\n",
    "\n",
    "2. Feature Extraction: The textual content of each email is preprocessed and transformed into feature vectors. Commonly used features include words, word frequencies, or more sophisticated features like n-grams (sequences of adjacent words). These features capture the presence or absence of specific terms or patterns in the email.\n",
    "\n",
    "3. Training the Classifier: The Naïve Bayes classifier is trained using the labeled training dataset. During training, the classifier estimates the probabilities of words or features occurring in spam and non-spam emails. It calculates the likelihood of each feature given each class (spam or non-spam).\n",
    "\n",
    "4. Calculating Prior Probabilities: The prior probabilities of spam and non-spam emails are calculated based on the proportion of each class in the training dataset. These prior probabilities represent the overall probability of an email being spam or non-spam, irrespective of the observed features.\n",
    "\n",
    "5. Calculating Posterior Probabilities: To classify new incoming emails, the Naïve Bayes classifier calculates the posterior probabilities of each class given the observed features. It applies Bayes' theorem to compute the probability of an email belonging to each class based on the occurrence of specific words or features in the email.\n",
    "\n",
    "6. Thresholding: The classifier assigns the most probable class label (spam or non-spam) to the email based on the computed posterior probabilities. A threshold can be set to determine the cutoff point for classifying an email as spam or non-spam. If the probability of being spam exceeds the threshold, the email is classified as spam.\n",
    "\n",
    "7. Evaluation and Refinement: The performance of the Naïve Bayes classifier is evaluated using metrics such as accuracy, precision, recall, and F1 score. Based on the evaluation results, the classifier can be refined by adjusting the feature selection, preprocessing techniques, or the threshold for classification.\n",
    "\n",
    "The Naïve Bayes classifier's ability to handle high-dimensional feature spaces and its efficiency make it well-suited for spam filtering. By learning from the features and their likelihoods, the classifier can effectively distinguish between spam and non-spam emails and classify incoming emails accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78a2adc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0e7acd5",
   "metadata": {},
   "source": [
    "#### 3. Market sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491761fd",
   "metadata": {},
   "source": [
    "The Naïve Bayes classifier is commonly used for market sentiment analysis, which involves determining the sentiment or opinion expressed in textual data related to financial markets, such as news articles, social media posts, or financial reports. Here's how the Naïve Bayes classifier is typically used for market sentiment analysis:\n",
    "\n",
    "1. Data Collection: Relevant textual data related to financial markets, such as news articles or social media posts, is collected from various sources. These texts may contain opinions, sentiments, or subjective expressions about specific companies, stocks, or market trends.\n",
    "\n",
    "2. Text Preprocessing: The collected textual data is preprocessed to remove noise and irrelevant information. This preprocessing may involve steps such as tokenization (splitting the text into individual words or tokens), removing punctuation, converting text to lowercase, and handling stop words (common words like \"and,\" \"the,\" etc.).\n",
    "\n",
    "3. Labeling the Data: A labeled dataset is created where each text is associated with a sentiment label, such as positive, negative, or neutral. Human annotators may manually label the texts based on the expressed sentiment or sentiment scores from external sources can be used for labeling.\n",
    "\n",
    "4. Feature Extraction: The textual data is transformed into feature vectors that can be used by the Naïve Bayes classifier. Commonly used features include words, word frequencies, or more advanced techniques like term frequency-inverse document frequency (TF-IDF). These features capture the presence or absence of specific words or phrases that indicate sentiment.\n",
    "\n",
    "5. Training the Classifier: The Naïve Bayes classifier is trained using the labeled dataset. During training, the classifier estimates the probabilities of words or features occurring in each sentiment class (positive, negative, or neutral). It calculates the likelihood of each feature given each sentiment class.\n",
    "\n",
    "6. Calculating Prior Probabilities: The prior probabilities of each sentiment class are calculated based on the proportion of each class in the training dataset. These prior probabilities represent the overall probability of a text belonging to each sentiment class, irrespective of the observed features.\n",
    "\n",
    "7. Calculating Posterior Probabilities: To classify new texts, the Naïve Bayes classifier calculates the posterior probabilities of each sentiment class given the observed features. It applies Bayes' theorem to compute the probability of a text belonging to each sentiment class based on the occurrence of specific words or features in the text.\n",
    "\n",
    "8. Sentiment Prediction: The classifier assigns the most probable sentiment label (positive, negative, or neutral) to the text based on the computed posterior probabilities. This predicted sentiment can then be used for further analysis, decision-making, or generating sentiment-based insights.\n",
    "\n",
    "9. Evaluation and Refinement: The performance of the Naïve Bayes classifier is evaluated using metrics such as accuracy, precision, recall, and F1 score. Based on the evaluation results, the classifier can be refined by adjusting the feature selection, preprocessing techniques, or other parameters to improve sentiment analysis accuracy.\n",
    "\n",
    "The Naïve Bayes classifier's simplicity, efficiency, and ability to handle high-dimensional feature spaces make it well-suited for market sentiment analysis. By learning from the features and their likelihoods, the classifier can effectively capture sentiment expressions in textual data, providing valuable insights for financial market analysis and decision-making."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
